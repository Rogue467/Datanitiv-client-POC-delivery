import numpy as np
import pandas as pd
df=pd.read_excel("C:\\Users\\sdprh\\Downloads\\brightcone\\DataForecasting\\cleaned_employee_data_5000_2.xlsx")
df.head(10)
print(df['Termination_date'][0])
for i in range(5000):
    if df['Attrition Status'][i]=="No":
        df['Termination_date'][i]="2024-09-05"

df.head(20)
import matplotlib.pyplot as plt
import seaborn as sns

print("Dataset Information:")
print(df.info())

print("\nSummary Statistics:")
print(df.describe())

print("\nDistribution of Categorical Features:")
print(df['Attrition Status'].value_counts())
print(df['Gender'].value_counts())
print(df['Marital Status'].value_counts())

df.hist(figsize=(12, 10), bins=30, edgecolor='black')
plt.suptitle('Histograms of Numerical Features')
plt.show()

plt.figure(figsize=(12, 8))
for i, column in enumerate(df.select_dtypes(include=['float64', 'int64']).columns):
    plt.subplot(3, 3, i+1)
    sns.boxplot(df[column])
    plt.title(f'Boxplot of {column}')
plt.tight_layout()
plt.show()

numeric_df = df.select_dtypes(include=['float64', 'int64'])
plt.figure(figsize=(12, 10))
corr = numeric_df.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()
from datetime import datetime

count = 0

for i in range(5000):
    joining_date = pd.to_datetime(df['Joining_date'][i])
    termination_date = pd.to_datetime(df['Termination_date'][i])
    
    difference = termination_date - joining_date

    df.at[i, 'Tenure (years)'] = round(difference.days / 365, 2)
    
    if df['Tenure (years)'][i] < 0:
        count += 1

print(count)
# indices=[]
# for i in range(5000):
#     if df['Tenure (years)'][i] < 0:
#         indices.append(i)
#     if df['Tenure (years)'][i]>df['Experience'][i]:
#         df['Tenure (years)'][i]=float(df['Experience'][i])
# df=df.drop(indices)
df.drop(['Emp_name','Joining_date','Termination_date'], axis=1, inplace=True)
df.head(10)
df = df.reset_index(drop=True)
df.head(10)
df['Attrition Status'] = df['Attrition Status'].map( {'Yes': 1, 'No': 0} ).astype(int)
df['Gender']=df['Gender'].map({'Male':1,'Female':0}).astype(int)
df['Marital Status']=df['Marital Status'].map({'Yes':1,'No':0}).astype(int)
df['Location']=df['Location'].map({'Hyderabad':1,'Noida':2,'Mumbai':3,'Pune':4,'Bangalore':5,'Gurugram':6,'Chennai':7,'Delhi':8,'Ahmedabad':9}).astype(int)
df.drop(['Location'], axis=1, inplace=True)
df.head(10)
y=df['Attrition Status']
x=df.drop(['Attrition Status'], axis=1)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y,random_state=1,test_size=0.3,shuffle=False)
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.metrics import accuracy_score
lr=LogisticRegression(C = 0.1, random_state = 42, solver = 'liblinear')
dt=DecisionTreeClassifier()
rm=RandomForestClassifier()
gnb=GaussianNB()
knn = KNeighborsClassifier(n_neighbors=3)
svm = svm.SVC(kernel='linear')
for a,b in zip([lr,dt,knn,svm,rm,gnb],["Logistic Regression","Decision Tree","KNN","SVM","Random Forest","Naive Bayes"]):
    a.fit(X_train,y_train)
    prediction=a.predict(X_train)
    y_pred=a.predict(X_test)
    score1=accuracy_score(y_train,prediction)
    score=accuracy_score(y_test,y_pred)
    msg1="[%s] training data accuracy is : %f" % (b,score1)
    msg2="[%s] test data accuracy is : %f" % (b,score)
    print(msg1)
    print(msg2)
model_scores={'Logistic Regression':lr.score(X_test,y_test),
             'KNN classifier':knn.score(X_test,y_test),
             'Support Vector Machine':svm.score(X_test,y_test),
             'Random forest':rm.score(X_test,y_test),
              'Decision tree':dt.score(X_test,y_test),
              'Naive Bayes':gnb.score(X_test,y_test)
             }
model_scores
from sklearn.metrics import classification_report

rm_y_preds = rm.predict(X_test)

print(classification_report(y_test,rm_y_preds))
from sklearn.metrics import classification_report

lr_y_preds = lr.predict(X_test)

print(classification_report(y_test,lr_y_preds))
model_compare=pd.DataFrame(model_scores,index=['accuracy'])
model_compare
model_compare.T.plot(kind='bar') # (T is here for transpose)
feature_dict=dict(zip(df.drop(['Attrition Status'], axis=1).columns,list(lr.coef_[0])))
feature_dict
feature_df=pd.DataFrame(feature_dict,index=[0])
feature_df.T.plot(kind="bar",legend=False,title="Feature Importance")
feature_df=pd.DataFrame(feature_dict,index=[0])
feature_df.T.plot(kind="bar",legend=False,title="Feature Importance")
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

y_pred = lr.predict(X_test)

# Calculate counts for actual and predicted values
actual_counts = pd.Series(y_test).value_counts().sort_index()
predicted_counts = pd.Series(y_pred).value_counts().sort_index()

# Create a DataFrame for the bar plot
comparison_counts = pd.DataFrame({
    'Attrition Status': ['No-Stay', 'Yes-Left'],
    'Predicted Count': predicted_counts.reindex([0, 1], fill_value=0).values,
    'Actual Count': actual_counts.values
})

# Plotting the grouped bar plot
fig, ax = plt.subplots(figsize=(12, 6))

# Plot bars
comparison_counts.set_index('Attrition Status').plot(kind='bar', ax=ax)

# Add annotations
for container in ax.containers:
    ax.bar_label(container, label_type='edge')

plt.title('Actual vs Predicted Attrition Status')
plt.xlabel('Attrition Status')
plt.ylabel('Number of Employees')
plt.xticks(rotation=0)
plt.legend(title='Count Type')
plt.show()
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

y_pred = lr.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

cm_df = pd.DataFrame(cm, index=['No', 'Yes'], columns=['No', 'Yes'])

plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False, linewidths=.5)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
import joblib

# saving our model - model - model , filename - model_lr
joblib.dump(lr , 'model_lr')

# opening the file- model_jlib
m_jlib1 = joblib.load('model_lr')

l1=list(m_jlib1.predict(X_test))
print(l1.count(1))
import joblib

# saving our model - model - model , filename - model_lr
joblib.dump(rm , 'model_rm')

# opening the file- model_jlib
m_jlib = joblib.load('model_rm')

l=list(m_jlib.predict(X_test)) 
print(l.count(1))
X_test['Attrition Status_rm']=l

X_test['Attrition Status_lr']=l1
X_test.head(1000)
